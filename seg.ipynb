{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "arr=np.array([[[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]]])\n",
    "arr=arr.flatten()\n",
    "arr[3:6]=1\n",
    "\n",
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_oneshot(label):\n",
    "    new_label=[]\n",
    "    for i in range(len(label)):\n",
    "        arr=np.zeros((label[i][:,:,0].shape[0]*label[i][:,:,0].shape[1],38),np.uint64)\n",
    "        flat_label=label[i][:,:,0].flatten()\n",
    "        for x in range(len(flat_label)):\n",
    "            arr[x,flat_label[x]]=1\n",
    "        arr=np.reshape(arr,(label[i][:,:,0].shape[0],label[i][:,:,0].shape[1],38))\n",
    "        new_label.append(arr)\n",
    "    return new_label\n",
    "def img_resize(img):\n",
    "    for i in range(len(img)):\n",
    "        img[i]=cv2.resize(img[i], None, fx=0.4, fy=0.4, interpolation=cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_oneshot(label):\n",
    "    arr=np.zeros((label[:,:,0].shape[0]*label[:,:,0].shape[1],38),np.uint64)\n",
    "    flat_label=label[:,:,0].flatten()\n",
    "    for x in range(len(flat_label)):\n",
    "        arr[x,flat_label[x]]=1\n",
    "    arr=np.reshape(arr,(label[:,:,0].shape[0],label[:,:,0].shape[1],38))\n",
    "    return arr\n",
    "def im_resize(img):\n",
    "    img=cv2.resize(img, None, fx=0.4, fy=0.4, interpolation=cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label=[cv2.imread('data/train_labels/img13labels-'+str(i+1).zfill(6)+'.png') for i in range(800)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[l_oneshot(im_resize(cv2.imread('data/train_labels/img13labels-'+str(i+1).zfill(6)+'.png'))) for i in range(800)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=[im_resize(cv2.imread('data/train_images/img-'+str(i+1).zfill(6)+'.jpg')) for i in range(800)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=[im_resize(cv2.imread('data/test_images/img-'+str(i+1).zfill(6)+'.jpg')) for i in range(500)]#5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=[l_oneshot(im_resize(cv2.imread('data/test_labels/img13labels-'+str(i+1).zfill(6)+'.png'))) for i in range(500)]#5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_label=[cv2.imread('data/test_labels/img13labels-'+str(i+1).zfill(6)+'.png') for i in range(500)]#5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image=img_resize(train_image)\n",
    "# train_label=img_resize(train_label)\n",
    "# test_label=img_resize(test_label)\n",
    "# test_image=img_resize(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 292, 38)\n",
      "61904\n"
     ]
    }
   ],
   "source": [
    "print(train_label[0].shape)\n",
    "print(len(train_label[0][:,:,0].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test_label=label_oneshot(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 292, 38)\n"
     ]
    }
   ],
   "source": [
    "print(test_label[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_train_label=label_oneshot(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "h=int(train_image[0].shape[0])\n",
    "w=int(train_image[0].shape[1])\n",
    "print(len(train_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 292, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learning_rate=0.0001\n",
    "training_epochs=100\n",
    "batch_size = 8\n",
    "tf.reset_default_graph()\n",
    "x=tf.placeholder(tf.float32,[None,h,w,3])\n",
    "y=tf.placeholder(tf.float32, [None,h,w,38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 212, 292, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_1_1=tf.get_variable(\"w1_1_1\", shape=[3,3,3,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w1_1_2=tf.get_variable(\"w1_1_2\", shape=[6,6,3,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w1_2_1=tf.get_variable(\"w1_2_1\", shape=[3,3,64,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w1_2_2=tf.get_variable(\"w1_2_2\", shape=[6,6,64,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "w2_1_1=tf.get_variable(\"w2_1_1\", shape=[3,3,64,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w2_1_2=tf.get_variable(\"w2_1_2\", shape=[6,6,64,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w2_2_1=tf.get_variable(\"w2_2_1\", shape=[3,3,128,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w2_2_2=tf.get_variable(\"w2_2_2\", shape=[6,6,128,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "w3_1_1=tf.get_variable(\"w3_1_1\", shape=[3,3,128,128], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_1_2=tf.get_variable(\"w3_1_2\", shape=[6,6,128,128], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_2_1=tf.get_variable(\"w3_2_1\", shape=[3,3,256,256], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_2_2=tf.get_variable(\"w3_2_2\", shape=[6,6,256,256], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_3_1=tf.get_variable(\"w3_3_1\", shape=[3,3,512,256], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_3_2=tf.get_variable(\"w3_3_2\", shape=[6,6,512,256], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "w3_4_1=tf.get_variable(\"w3_4_1\", shape=[3,3,512,128], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_4_2=tf.get_variable(\"w3_4_2\", shape=[6,6,512,128], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w3_4_3=tf.get_variable(\"w3_4_3\", shape=[3,3,256,256], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "\n",
    "w4_1_1=tf.get_variable(\"w4_1_1\", shape=[3,3,256,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w4_1_2=tf.get_variable(\"w4_1_2\", shape=[6,6,256,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w4_2_1=tf.get_variable(\"w4_2_1\", shape=[3,3,128,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w4_2_2=tf.get_variable(\"w4_2_2\", shape=[6,6,128,64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "w5_1_1=tf.get_variable(\"w5_1_1\", shape=[3,3,128,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w5_1_2=tf.get_variable(\"w5_1_2\", shape=[6,6,128,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w5_2_1=tf.get_variable(\"w5_2_1\", shape=[3,3,64,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "w5_2_2=tf.get_variable(\"w5_2_2\", shape=[6,6,64,32], initializer=tf.contrib.layers.xavier_initializer_conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_1_1=tf.Variable(tf.random_normal([32]))\n",
    "b1_1_2=tf.Variable(tf.random_normal([32]))\n",
    "b1_2_1=tf.Variable(tf.random_normal([32]))\n",
    "b1_2_2=tf.Variable(tf.random_normal([32]))\n",
    "\n",
    "b2_1_1=tf.Variable(tf.random_normal([64]))\n",
    "b2_1_2=tf.Variable(tf.random_normal([64]))\n",
    "b2_2_1=tf.Variable(tf.random_normal([64]))\n",
    "b2_2_2=tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "b3_1_1=tf.Variable(tf.random_normal([128]))\n",
    "b3_1_2=tf.Variable(tf.random_normal([128]))\n",
    "b3_2_1=tf.Variable(tf.random_normal([256]))\n",
    "b3_2_2=tf.Variable(tf.random_normal([256]))\n",
    "b3_3_1=tf.Variable(tf.random_normal([256]))\n",
    "b3_3_2=tf.Variable(tf.random_normal([256]))\n",
    "b3_4_1=tf.Variable(tf.random_normal([128]))\n",
    "b3_4_2=tf.Variable(tf.random_normal([128]))\n",
    "\n",
    "\n",
    "b4_1_1=tf.Variable(tf.random_normal([64]))\n",
    "b4_1_2=tf.Variable(tf.random_normal([64]))\n",
    "b4_2_1=tf.Variable(tf.random_normal([64]))\n",
    "b4_2_2=tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "b5_1_1=tf.Variable(tf.random_normal([32]))\n",
    "b5_1_2=tf.Variable(tf.random_normal([32]))\n",
    "b5_2_1=tf.Variable(tf.random_normal([32]))\n",
    "b5_2_2=tf.Variable(tf.random_normal([32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############1 conv layer\n",
    "l1_1_1=tf.nn.relu(tf.nn.conv2d(x,w1_1_1, strides=[1,1,1,1], padding= 'SAME')+b1_1_1)#(?,265,365,32)\n",
    "l1_1_2=tf.nn.relu(tf.nn.conv2d(x,w1_1_2, strides=[1,1,1,1], padding= 'SAME')+b1_1_2)#(?,265,365,32)\n",
    "l1_conv_1=tf.concat([l1_1_1, l1_1_2],3)#(?,265,365,64)\n",
    "l1_2_1=tf.nn.relu(tf.nn.conv2d(l1_conv_1,w1_2_1, strides=[1,1,1,1], padding= 'SAME')+b1_2_1)#(?,265,365,32)\n",
    "l1_2_2=tf.nn.relu(tf.nn.conv2d(l1_conv_1,w1_2_2, strides=[1,1,1,1], padding= 'SAME')+b1_2_2)#(?,265,365,32)\n",
    "l1_conv_2=tf.concat([l1_2_1,l1_2_2],3)#(?,265,365,64)\n",
    "l1=tf.nn.max_pool(l1_conv_2, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')#(?,89, 122,,64)\n",
    "\n",
    "#############2 conv layer\n",
    "l2_1_1=tf.nn.relu(tf.nn.conv2d(l1,w2_1_1, strides=[1,1,1,1], padding= 'SAME')+b2_1_1)#(?,89, 122,64)\n",
    "l2_1_2=tf.nn.relu(tf.nn.conv2d(l1,w2_1_2, strides=[1,1,1,1], padding= 'SAME')+b2_1_2)#(?,89, 122,64)\n",
    "l2_conv_1=tf.concat([l2_1_1, l2_1_2],3)#(?,88,121,128)\n",
    "l2_2_1=tf.nn.relu(tf.nn.conv2d(l2_conv_1,w2_2_1, strides=[1,1,1,1], padding= 'SAME')+b2_2_1)#(?,89, 122,64)\n",
    "l2_2_2=tf.nn.relu(tf.nn.conv2d(l2_conv_1,w2_2_2, strides=[1,1,1,1], padding= 'SAME')+b2_2_2)#(?,89, 122,64)\n",
    "l2_conv_2=tf.concat([l2_2_1, l2_2_2],3)#(?,88,121,128)\n",
    "l2=tf.nn.max_pool(l2_conv_2, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')#(?,30, 41,128)\n",
    "\n",
    "#############3 conv layer\n",
    "l3_1_1=tf.nn.relu(tf.nn.conv2d(l2,w3_1_1, strides=[1,1,1,1], padding= 'SAME')+b3_1_1)#(?,30, 41, 128)\n",
    "l3_1_2=tf.nn.relu(tf.nn.conv2d(l2,w3_1_2, strides=[1,1,1,1], padding= 'SAME')+b3_1_2)#(?,30, 41, 128)\n",
    "l3_conv_1=tf.concat([l3_1_1, l3_1_2],3)#(?,30, 41, 256)\n",
    "l3_2_1=tf.nn.relu(tf.nn.conv2d(l3_conv_1,w3_2_1, strides=[1,1,1,1], padding= 'SAME')+b3_2_1)#(?,30, 41, 256)\n",
    "l3_2_2=tf.nn.relu(tf.nn.conv2d(l3_conv_1,w3_2_2, strides=[1,1,1,1], padding= 'SAME')+b3_2_2)#(?,30, 41, 256)\n",
    "l3_conv_2=tf.concat([l3_2_1, l3_2_2],3)#(?,30, 41, 512)\n",
    "l3_3_1=tf.nn.relu(tf.nn.conv2d(l3_conv_2,w3_3_1, strides=[1,1,1,1], padding= 'SAME')+b3_3_1)#(?,30, 41, 256)\n",
    "l3_3_2=tf.nn.relu(tf.nn.conv2d(l3_conv_2,w3_3_2, strides=[1,1,1,1], padding= 'SAME')+b3_3_2)#(?,30, 41, 256)\n",
    "l3_conv_3=tf.concat([l3_3_1, l3_3_2],3)#(?,30, 41, 512)\n",
    "l3=tf.nn.max_pool(l3_conv_2, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')#(?,30, 41,512)\n",
    "\n",
    "#############4 conv layer\n",
    "l3_4_1=tf.nn.relu(tf.nn.conv2d(l3,w3_4_1, strides=[1,1,1,1], padding= 'SAME')+b3_4_1)#(?,30, 41, 128)\n",
    "l3_4_2=tf.nn.relu(tf.nn.conv2d(l3,w3_4_2, strides=[1,1,1,1], padding= 'SAME')+b3_4_2)#(?,30, 41, 128)\n",
    "l3_1=tf.concat([l3_4_1, l3_4_2],3)#(?,30, 41, 256)\n",
    "l3_2=tf.image.resize_images(l3_1,[int(math.ceil(h/9)),int(math.ceil(w/9))])#(?,10, 12, 256)\n",
    "l3_3=tf.nn.relu(tf.nn.conv2d_transpose(l3_2,w3_4_3,tf.stack([tf.shape(x)[0],int(math.ceil(h/9)),int(math.ceil(w/9)),256]),strides=[1,1,1,1],padding='SAME'))\n",
    "\n",
    "#############1 deconv layer\n",
    "l4_1=tf.image.resize_images(l3_3,[int(math.ceil(h/3)),int(math.ceil(w/3))])#(?,89, 122, 256)\n",
    "l4_1_1=tf.nn.relu(tf.nn.conv2d(l4_1,w4_1_1, strides=[1,1,1,1], padding= 'SAME')+b4_1_1)#(?,89, 122, 64)\n",
    "l4_1_2=tf.nn.relu(tf.nn.conv2d(l4_1,w4_1_2, strides=[1,1,1,1], padding= 'SAME')+b4_1_2)#(?,89, 122, 64)\n",
    "l4_conv_1=tf.concat([l4_1_1, l4_1_2],3)#(?,89, 122, 128)\n",
    "l4_2_1=tf.nn.relu(tf.nn.conv2d(l4_conv_1,w4_2_1, strides=[1,1,1,1], padding= 'SAME')+b4_2_1)#(?,89, 122, 64)\n",
    "l4_2_2=tf.nn.relu(tf.nn.conv2d(l4_conv_1,w4_2_2, strides=[1,1,1,1], padding= 'SAME')+b4_2_2)#(?,89, 122, 64)\n",
    "l4=tf.concat([l4_2_1, l4_2_2],3)#(?,89, 122, 128)\n",
    "\n",
    "#############2 deconv layer\n",
    "l5_1=tf.image.resize_images(l4,[h,w])#(?,265,365, 128)\n",
    "l5_1_1=tf.nn.relu(tf.nn.conv2d(l5_1,w5_1_1, strides=[1,1,1,1], padding= 'SAME')+b5_1_1)#(?,265,365, 32)\n",
    "l5_1_2=tf.nn.relu(tf.nn.conv2d(l5_1,w5_1_2, strides=[1,1,1,1], padding= 'SAME')+b5_1_2)#(?,265,365, 32)\n",
    "l5_conv_1=tf.concat([l5_1_1, l5_1_2],3)#(?,265,365, 64)\n",
    "l5_2_1=tf.nn.relu(tf.nn.conv2d(l5_conv_1,w5_2_1, strides=[1,1,1,1], padding= 'SAME')+b5_2_1)#(?,265,365, 32)\n",
    "l5_2_2=tf.nn.relu(tf.nn.conv2d(l5_conv_1,w5_2_2, strides=[1,1,1,1], padding= 'SAME')+b5_2_2)#(?,265,365, 32)\n",
    "l5=tf.concat([l5_2_1, l5_2_2],3)#(?,265,365, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 71, 98, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 24, 33, 128), dtype=float32)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 8, 11, 512), dtype=float32)\n",
      "Tensor(\"concat_9:0\", shape=(?, 71, 98, 128), dtype=float32)\n",
      "Tensor(\"concat_10:0\", shape=(?, 212, 292, 64), dtype=float32)\n",
      "Tensor(\"ResizeBilinear_1:0\", shape=(?, 71, 98, ?), dtype=float32)\n",
      "Tensor(\"Relu_16:0\", shape=(?, ?, ?, ?), dtype=float32)\n",
      "24 33\n"
     ]
    }
   ],
   "source": [
    "print(l1)\n",
    "print(l2)\n",
    "print(l3)\n",
    "print(l4)\n",
    "print(l5)\n",
    "print(l4_1)\n",
    "print(l3_3)\n",
    "print(int(math.ceil(h/9)),int(math.ceil(w/9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "w6=tf.get_variable(\"w6\", shape=[1,1,64,128], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "b6=tf.Variable(tf.random_normal([128]))\n",
    "l6=tf.nn.relu(tf.nn.conv2d(l5,w6, strides=[1,1,1,1], padding= 'SAME')+b6)#(?,265,365,512)\n",
    "\n",
    "w7=tf.get_variable(\"w7\", shape=[1,1,128,38], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "b7=tf.Variable(tf.random_normal([38]))\n",
    "l7=tf.nn.conv2d(l6,w7, strides=[1,1,1,1], padding= 'SAME')+b7#(?,265,365,37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_12:0\", shape=(?, 212, 292, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(l6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=tf.reshape(l7,[-1,38])\n",
    "soft=tf.nn.softmax(output)\n",
    "flat_y=tf.reshape(y,[-1,38])\n",
    "\n",
    "\n",
    "entropy=-tf.log(soft)*flat_y\n",
    "t_cost=tf.reshape(entropy,[-1,h*w*38])\n",
    "a_loss=tf.reduce_mean(t_cost,1)\n",
    "cost=tf.reduce_mean(t_cost)\n",
    "\n",
    "#cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=flat_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l10_1=tf.reshape(l10,[-1,265*365,37])\n",
    "# out=[]\n",
    "\n",
    "# for i in range(265*365):\n",
    "#     out.append(tf.nn.softmax(l10_1[:,i,:]))\n",
    "#     if i%10000 ==0:\n",
    "#         print(i)\n",
    "# logits=tf.stack(out)\n",
    "# logits=tf.transpose(logits,perm=[1,0,2])\n",
    "# logits=tf.reshape(logits,[-1,265,365,37])\n",
    "# print(logits)\n",
    "# cost=tf.log(logits)*y\n",
    "# print(cost)\n",
    "# # cost=tf.reduce_mean(tf.reshape(cost,[-1,265*365*37]),1)\n",
    "# # total_cost=tf.reduce_mean(cost)\n",
    "# # print(total_cost)\n",
    "# total_cost=tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=tf.train.exponential_decay(0.001,batch_size*batch_size,training_epochs,0.9)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "image=tf.argmax(output,3)\n",
    "y_image=tf.argmax(y,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start\n",
      "epoch :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning start\")\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    a=0\n",
    "    total_batch = int(math.ceil(len(train_image)/batch_size))\n",
    "    print(\"epoch : \",epoch)\n",
    "    for i in range(total_batch):\n",
    "        if a+batch_size>len(train_image):\n",
    "            batch_xs=train_image[a:]\n",
    "            batch_ys=train_label[a:]\n",
    "        else:\n",
    "            batch_xs=train_image[a:a+batch_size]\n",
    "            batch_ys=train_label[a:a+batch_size]\n",
    "        feed_dict={x:batch_xs, y:batch_ys}\n",
    "        y_img,img,l,_=sess.run([y_image,image,cost,optimizer], feed_dict=feed_dict)\n",
    "        if l==tf.is_nan:\n",
    "            break\n",
    "        for i in range(len(batch_xs)):\n",
    "            im=np.minimum(img[i,:,:]*5,255)\n",
    "            cv2.imwrite('output/output_img-'+str(a+i+1).zfill(6)+'.jpg',im)\n",
    "\n",
    "        a=a+batch_size\n",
    "    \n",
    "    print(\"loss : \", l)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    y_im=np.minimum(y_img[0,:,:]*5,255)\n",
    "    im1=np.minimum(img[0,:,:]*5,255)\n",
    "    plt.subplot(1,2,1); plt.imshow(y_im)       \n",
    "    plt.subplot(1,2,2); plt.imshow(im1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=sess.run([image],feed_dict={x:test_image, y:new_test_label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-754532db901a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcor_pre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcor_pre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_conv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "cor_pre=tf.equal(tf.arg_max(output,1),tf.arg_max(y,1))\n",
    "acc_=tf.reduce_mean(tf.cast(cor_pre,tf.float32))\n",
    "feed_dict={x1:test_image, y:test_label, keep_conv:0.8, keep_hidden:0.5}\n",
    "test_acc=sess.run(acc_,feed_dict=feed_dict)\n",
    "print(\"accuracy : \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
